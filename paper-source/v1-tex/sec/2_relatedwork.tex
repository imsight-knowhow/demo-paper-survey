\section{Related work}
\label{sec:intro}
\paragraph{\textbf{Efficient Diffusion Models}}
Diffusion models \cite{song2020score,ho2020denoising,rombach2022high,Peebles2022DiT} have sparked considerable interest in the commercial sector due to their high-quality image/video generation capabilities. These models have been progressively adapted for various applications, including text-to-image generation \cite{rombach2021highresolution,ramesh2022hierarchical,avrahami2023blendedlatent}, image editing \cite{Avrahami_2022_CVPR,ruiz2022dreambooth}, video generation \cite{blattmann2023stable,blattmann2023align} and even perception \cite{xu20233difftection,li2023grounded,khani2024slime}. However, diffusion models are currently limited by their slow speed in generating outputs.
In response to this challenge, a variety of strategies have been proposed. One of the mainstreams is to approximate the SDE-based diffusion process \cite{song2020score,song2021denoising} through an ordinary differentiable equation (ODE) framework. For example, DPM and DPM++ \cite{lu2022dpm,lu2022dpm+} introduce ODE-based samplers, which significantly reduce the hundreds of denoising steps to between 15 and 20. Building upon the ODE formulation, InstaFlow \cite{liu2023insta} advances the reduction of denoising steps to a single instance through the novel strategy of rectified flow \cite{liu2022flow}, while achieving performance close to that of Stable Diffusion \cite{rombach2022high}. Additionally, distillation from pre-trained diffusion models has also been explored as a method to facilitate few-step denoising. For instance, the consistency model \cite{song2023consistency} leverages the principle of self-consistency between noise at different denoising steps and uses pre-trained diffusion models \cite{rombach2022high} to guide the learning of a few-step denoising model, thereby enabling the generation of images within a minimal number of steps. In a notable extension of this concept, LCM \cite{luo2023lcm,luo2023lcmlora} applies the idea to the latent space rather than the pixel space. The use of distillation methods \cite{sauer2023adversarial,salimans2022progressive,yin2023onestep} to enhance the efficiency of the original Stable Diffusion model presents promising results.
Besides improving efficiency through the lens of reducing denoising steps, quantization methods \cite{li2023qdiffusion,huang2023tfmq} are proposed to make the model run in the regime of low float points, albeit with the potential trade-off of fidelity and efficiency.
Moreover, parallel sampling \cite{shih2024parallel} tries to utilize the approximate-parallel denoising strategy to improve the latency of the diffusion model. We emphasize that our work focuses on enhancing throughput and our work is orthogonal to \cite{shih2024parallel}. Notably, our method is optimized for single-GPU, which are common among users. In contrast, the parallel sampling method reduces throughput on a single GPU. 

Our proposed StreamDiffusion is significantly different from the approaches mentioned previously. While earlier methods primarily focus on the low latency of their individual model designs, our approach takes a different route. We introduce a \textit{pipeline-level} solution specifically tailored for high throughput. Our pipeline can seamlessly integrate the low-latency diffusion models discussed above. Our proposed Stream Batch, residual classifier-free guidance, and the integration of other efficiency-enhancement methods focus on improving the efficiency of the whole pipeline instead of a single diffusion model. 


\begin{figure}[!t]
  \centering
   \includegraphics[width=1.0\linewidth]{figure/batch_denoising_concept.png}
\caption{The concept of Stream Batch. In our approach, instead of waiting for a single image to be fully denoised before processing the next input image, we accept the next input image after each denoising step. This creates a denoising batch where the denoising steps are staggered for each image. By concatenating these staggered denoising steps into a batch, we can efficiently process continuous inputs using a U-Net for batch processing. The input image encoded at timestep $t$ is generated and decoded at timestep $t+n$, where $n$ is the number of the denoising steps.}
\label{fig:batch_concept}
\end{figure}

\paragraph{\textbf{Classifier-free Guidance}}
Classifier-free guidance \cite{ho2022classifier} is widely used for conditional generation due to the simplicity, efficiency, and stability compared to classifier-guidance \cite{dhariwal2021diffusion}. It leverages negative prompts \cite{crowson2022vqgan,du2020compositional,rombach2022high} and essentially operates the vector arithmetic shift
in latent space, \textit{i.e.,} we take a step of size (usually set by the guidance scale) away from the unconditional vector or negatively conditioned vector in the
direction toward the conditioning manifold \cite{ho2022classifier}. In the practical implementation, the classifier-free guidance is conducted by sharing the same UNet for both the conditional term and unconditional (or negative) term and subtracting the effect of the unconditional term from the conditioned one. We point out that the way of multiple denoising processes leads to unnecessary computations for the unconditional (or negative) term. To get rid of these redundant computations, we propose a novel residual-classifier-free guidance, termed as \textbf{R-CFG}, which approximates the conditional noise prediction with only requiring one or even zero-time computation for the negatively conditioned noise prediction for the UNet. We note that \textit{R-CFG} is especially designed for the SDEdit method \cite{meng2022sdedit}, as we mainly focus on the applications of translating streaming image flow. Our proposed R-CFG significantly improves the latency of the conditional image-to-image generation.

